% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/trainBrt.r
\name{trainBrt}
\alias{trainBrt}
\title{Calibrate a boosted regression tree (generalized boosting machine) model}
\usage{
trainBrt(
  data,
  resp = names(data)[1],
  preds = names(data)[2:ncol(data)],
  family = "bernoulli",
  learningRate = c(1e-04, 0.001, 0.01),
  treeComplexity = c(5, 3, 1),
  bagFraction = 0.6,
  minTrees = 1000,
  maxTrees = 8000,
  tries = 5,
  tryBy = c("learningRate", "treeComplexity", "maxTrees", "stepSize"),
  w = TRUE,
  anyway = FALSE,
  out = "model",
  cores = 1,
  verbose = FALSE,
  ...
)
}
\arguments{
\item{data}{data frame with first column being response}

\item{resp}{Character or integer. Name or column index of response variable. Default is to use the first column in \code{data}.}

\item{preds}{Character list or integer list. Names of columns or column indices of predictors. Default is to use the second and subsequent columns in \code{data}.}

\item{family}{Character. Name of error family.  See \code{\link[dismo]{gbm.step}}.}

\item{learningRate}{Numeric. Learning rate at which model learns from successive trees (Elith et al. 2008 recommend 0.0001 to 0.1).}

\item{treeComplexity}{Positive integer. Tree complexity: depth of branches in a single tree (1 to 16).}

\item{bagFraction}{Numeric in the range [0, 1]. Bag fraction: proportion of data used for training in cross-validation (Elith et al. 2008 recommend 0.5 to 0.7).}

\item{minTrees}{Positive integer. Minimum number of trees to be scored as a "usable" model (Elith et al. 2008 recommend at least 1000). Default is 1000.}

\item{maxTrees}{Positive integer. Maximum number of trees in model set (same as parameter \code{max.trees} in \code{\link[dismo]{gbm.step}}).}

\item{tries}{Integer > 0. Number of times to try to train a model with a particular set of tuning parameters. The function will stop training the first time a model converges (usually on the first attempt). Non-convergence seems to be related to the number of trees tried in each step.  So if non-convergence occurs then the function automatically increases the number of trees in the step size until \code{tries} is reached.}

\item{tryBy}{Character list. A list that contains one or more of \code{'learningRate'}, \code{'treeComplexity'}, \code{numTrees}, and/or \code{'stepSize'}. If a given combination of \code{learningRate}, \code{treeComplexity}, \code{numTrees}, \code{stepSize}, and \code{bagFraction} do not allow model convergence then then the function tries again but with alterations to any of the arguments named in \code{tryBy}:
* \code{learningRate}: Decrease the learning rate by a factor of 10.
* \code{treeComplexity}: Randomly increase/decrease tree complexity by 1 (minimum of 1).
* \code{maxTrees}: Increase number of trees by 20%.
* \code{stepSize}: Increase step size (argument \code{n.trees} in \code{gbm.step()}) by 50%.
If \code{tryBy} is NULL then the function attempts to train the model with the same parameters up to \code{tries} times.}

\item{w}{Either logical in which case \code{TRUE} (default) causes the total weight of presences to equal the total weight of absences (if \code{family='binomial'}) \emph{or} a numeric list of weights, one per row in \code{data} \emph{or} the name of the column in \code{data} that contains site weights. If \code{FALSE}, then each datum gets a weight of 1.}

\item{anyway}{Logical. If \code{FALSE} (default), it is possible for no models to be returned if none converge and/or none had a number of trees is >= \code{minTrees}). If \code{TRUE} then all models are returned but with a warning.}

\item{out}{Character. Indicates type of value returned. If \code{model} (default) then returns an object of class \code{gbm}. If \code{models} then all models that were trained are returned in a list in the order they appear in the tuning table (this may take a lot of memory!). If \code{tuning} then just return a data frame with tuning parameters and deviance of each model sorted by deviance. If both then return a 2-item list with the best model and the tuning table.}

\item{cores}{Integer >= 1. Number of cores to use when calculating multiple models. Default is 1.}

\item{verbose}{Logical. If \code{TRUE} display progress.}

\item{...}{Arguments to pass to \code{\link[dismo]{gbm.step}}.}
}
\value{
If \code{out = 'model'} this function returns an object of class \code{gbm}. If \code{out = 'tuning'} this function returns a data frame with tuning parameters and cross-validation deviance for each model tried. If \code{out = c('model', 'tuning'} then it returns a list object with the \code{gbm} object and the data frame. Note that if a model does not converge or does not meet sufficiency criteria (i.e., the number of optimal trees is < \code{minTrees}, then the model is not returned (a \code{NULL} value is returned for \code{'model'} and models are simply missing from the \code{tuning} and \code{models} output.
}
\description{
This function is a wrapper for \code{\link[dismo]{gbm.step}}. It returns the model with best combination of learning rate, tree depth, and bag fraction based on cross-validated deviance. It can also return a table with deviance of different combinations of tuning parameters that were tested, and all of the models tested. See Elith, J., J.R. Leathwick, and T. Hastie. 2008. A working guide to boosted regression trees. \emph{Journal of Animal Ecology} 77:802-813.
}
\examples{
\dontrun{
### model red-bellied lemurs
data(mad0)
data(lemurs)

# climate data
bios <- c(1, 5, 12, 15)
clim <- raster::getData('worldclim', var='bio', res=10)
clim <- raster::subset(clim, bios)
clim <- raster::crop(clim, mad0)

# occurrence data
occs <- lemurs[lemurs$species == 'Eulemur rubriventer', ]
occsEnv <- raster::extract(clim, occs[ , c('longitude', 'latitude')])

# background sites
bg <- 2000 # too few cells to locate 10000 background points
bgSites <- dismo::randomPoints(clim, 2000)
bgEnv <- raster::extract(clim, bgSites)

# collate
presBg <- rep(c(1, 0), c(nrow(occs), nrow(bgSites)))
env <- rbind(occsEnv, bgEnv)
env <- cbind(presBg, env)
env <- as.data.frame(env)

preds <- paste0('bio', bios)

# settings... defaults probably better, but these are faster
lr <- c(0.001, 0.1)
tc <- c(1, 3)
maxTrees <- 2000
set.seed(123)
model <- trainBrt(
	data = env,
	resp = 'presBg',
	preds = preds,
	learningRate = lr,
	treeComplexity = tc,
	maxTrees = maxTrees,
	verbose = TRUE
)

plot(model)

# prediction raster
nTrees <- model$gbm.call$n.trees
map <- predict(clim, model, type='response', n.trees=nTrees)
plot(map)
points(occs[ , c('longitude', 'latitude')])

}
}
\seealso{
\code{\link[dismo]{gbm.step}}
}
